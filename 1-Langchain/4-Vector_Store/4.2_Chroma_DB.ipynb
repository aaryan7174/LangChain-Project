{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9142186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2010236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='My parents impressed on me the value of that you work hard for what you want in life. \\nThat your word is your bond and you do what you say and keep your promise. \\nThat you treat people with respect. \\nShow the values and morals in in the daily life. \\nThat is the lesson that we continue to pass on to our son.\\nWe need to pass those lessons on to the many generations to follow. \\n[Cheering] Because we want our children in these nations to know that the only limit to your achievement is the strength of your dreams and your willingness to work for them.\\nNow is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  docs = text_splitter.split_documents(documents)\\nğŸ•µï¸ Whatâ€™s Likely Going Wrong?\\nThe chunk_size=20 with chunk_overlap=5 is super small. You\\'re telling it to chop the text into tiny 20-character pieces, overlapping by 5 characters. But since your text isnâ€™t massive, thatâ€™s likely resulting in only a few chunksâ€”maybe even less than 5.\\n\\nPlus, it splits based on character count, not sentences or paragraphs.\\n\\nğŸ§ª Try This Instead: Use Larger Chunks!\\nLetâ€™s beef it up a bit:\\n\\nPython\\nCopy\\nEdit\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_text_splitters import CharacterTextSplitter\\n\\n# Load speech\\nloader = TextLoader(\"Speech.txt\")\\ndocuments = loader.load()\\n\\n# Use larger chunks to get more meaningful pieces\\ntext_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\\ndocs = text_splitter.split_documents(documents)\\n\\n# Check number of splits\\nprint(f\"Total chunks: {len(docs)}\")\\nfor i, doc in enumerate(docs):\\n    print(f\"\\\\n--- Chunk {i+1} ---\\\\n{doc.page_content}\")\\nğŸ’¡ Want Even Smarter Splitting?\\nIf you\\'re aiming for chunks that make more natural sense (like sentences or paragraphs), try using:\\n\\nPython\\nCopy\\nEdit\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=200,\\n    chunk_overlap=50,\\n    separators=[\"\\\\n\\\\n\", \"\\\\n\", \".\", \" \"]\\n)\\ndocs = text_splitter.split_documents(documents)\\nThis will keep your splits a bit more human-readable and context-aware!\\n\\nâœ… Summary:\\nchunk_size=20 is too smallâ€”use 200+ for better results.\\n\\nUse RecursiveCharacterTextSplitter if you want natural sentence/paragraph breaks.\\n\\nsplit_documents() will always return a list of Document objects, even for one file.\\n\\nIf you want to split multiple files, use DirectoryLoader instead of TextLoader. Let me know if you need that too!\\n\\n\\n\\n\\n\\n\\n\\nYou said:\\naywhat does chunk\\n\\n\\nğŸ§  Real-Life Analogy: Netflix Series ğŸ¬\\nImagine your speech is a story, and you want to split it into episodes.\\n\\nchunk_size = how long each episode is\\n\\nchunk_overlap = the â€œPreviously onâ€¦â€ intro at the beginning of each new episode\\n\\nğŸ” Why overlap matters?\\nIf an important sentence is cut in the middle, overlap helps the next chunk pick it up smoothly so the model doesnâ€™t get confused.\\n\\nğŸ§ª Example:\\nLetâ€™s say we have this short text:\\n\\nâ€œMy parents impressed on me the value that you work hard for what you want in life.â€\\n\\nCase 1: Small chunk size, no overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\nChunks:\\n\\nâ€œMy parents impressâ€\\n\\nâ€œed on me the valueâ€\\n\\nâ€œ that you work harâ€\\n\\nâ€œd for what you wanâ€\\n\\nâ€œt in life.â€\\n\\nResult: You may lose meaning because words/sentences are broken.\\n\\nCase 2: Bigger chunk with overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 40\\nchunk_overlap = 10\\nChunks:\\n\\nâ€œMy parents impressed on me the value that yoâ€\\n\\nâ€œvalue that you work hard for what you want â€\\n\\nâ€œwhat you want in life.â€\\n\\nNow, chunks carry better continuity and preserve meaning.\\n\\nğŸ› ï¸ When to Use What?\\nUse Case\\tchunk_size\\tchunk_overlap\\nTiny docs\\tsmall (100â€“200)\\t10â€“20\\nLong speeches/books\\tbig (500â€“1000)\\t50â€“100\\nQA over documents\\tmedium (300â€“600)\\t50\\nSentence-sensitive tasks\\tsmall\\tmedium\\n\\nğŸ§ TL;DR â€“ Key Takeaways\\nchunk_size = how big each text piece is\\n\\nchunk_overlap = how much text is shared between chunks\\n\\nSmall chunks: easier to process, but may lose context\\n\\nBig chunks: preserve meaning, but take more memory\\n\\nOverlap: preserves flow between chunks\\n\\nLet me know if you want a visual image or diagram to understand it betterâ€”Iâ€™ll whip one up! ğŸ˜„\\n\\n\\n\\n\\n\\n\\n\\nYou said:\\nCase 1: Small chunk size, no overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\nChunks:\\n\\nâ€œMy parents impressâ€\\n\\nâ€œed on me the valueâ€\\n\\nâ€œ that you work harâ€\\n\\nâ€œd for what you wanâ€\\n\\nâ€œt in life.â€I don\\'t think the chunk size here is 20.\\n\\n\\nChatGPT said:\\nYou\\'re absolutely right, detective! ğŸ•µï¸\\u200dâ™‚ï¸ Great catch â€” that example did not accurately show chunks of size 20 characters. Let\\'s fix that with real and precise math like good AI engineers ğŸ˜.\\n\\nğŸ§® Corrected Example:\\nLetâ€™s take this sentence:\\n\\n\"My parents impressed on me the value that you work hard for what you want in life.\"\\n\\nThis sentence has 92 characters (including spaces and punctuation).\\n\\nNow letâ€™s split it correctly using:\\n\\npython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\nâœ… Real Chunks (each = 20 characters):\\n\"My parents impresse\" â†’ 20 characters\\n\\n\"d on me the value t\" â†’ next 20 characters\\n\\n\"hat you work hard f\"\\n\\n\"or what you want in\"\\n\\n\" life.\" â†’ last remaining characters (less than 20)\\n\\nâœ… Now itâ€™s accurate!\\n\\nâš ï¸ Why It Matters\\nWhen you\\'re using CharacterTextSplitter, it slices your text based on character count, not words or sentences â€” unless you use other splitters like RecursiveCharacterTextSplitter or SentenceTextSplitter.\\n\\nSo, setting chunk_size=20 tells it:\\n\\nâ€œHey LangChain, give me 20 characters per slice, and donâ€™t repeat anything if overlap = 0.â€\\n\\nğŸ’¡ Pro Tip\\nTo split by sentences or smartly, use this instead:\\n\\npython\\nCopy\\nEdit\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=100,\\n    chunk_overlap=20,\\n    separators=[\"\\\\n\\\\n\", \"\\\\n\", \".\", \"!\", \"?\", \" \", \"\"]\\n)\\nThis tries to break at sentence ends or spaces, avoiding weird cuts in the middle of words like \"value t\".\\nLet me know if you\\'d like to compare word-based or sentence-based chunking too! Iâ€™ll chunk your worries away ğŸ˜„âœ¨\\n\\nTools\\nChatGPT can make mistakes. Check important info. See Cookie Preferences.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508fd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "split = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba437387",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\", model_kwargs={\"temperature\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03dacf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=split,\n",
    "    embedding=embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19277209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1204e7f70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a86b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ğŸ› ï¸ When to Use What?\\nUse Case\\tchunk_size\\tchunk_overlap\\nTiny docs\\tsmall (100â€“200)\\t10â€“20\\nLong speeches/books\\tbig (500â€“1000)\\t50â€“100\\nQA over documents\\tmedium (300â€“600)\\t50\\nSentence-sensitive tasks\\tsmall\\tmedium\\n\\nğŸ§ TL;DR â€“ Key Takeaways\\nchunk_size = how big each text piece is\\n\\nchunk_overlap = how much text is shared between chunks\\n\\nSmall chunks: easier to process, but may lose context\\n\\nBig chunks: preserve meaning, but take more memory\\n\\nOverlap: preserves flow between chunks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the main topic of the speech?\"\n",
    "docs = vector_db.similarity_search(query, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d78ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db=Chroma.from_documents(documents=split, embedding=embedding, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46210f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
