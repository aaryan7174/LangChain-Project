{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9142186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings \n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2010236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='My parents impressed on me the value of that you work hard for what you want in life. \\nThat your word is your bond and you do what you say and keep your promise. \\nThat you treat people with respect. \\nShow the values and morals in in the daily life. \\nThat is the lesson that we continue to pass on to our son.\\nWe need to pass those lessons on to the many generations to follow. \\n[Cheering] Because we want our children in these nations to know that the only limit to your achievement is the strength of your dreams and your willingness to work for them.\\nNow is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  Now is the winter of our discontent\\n  Made glorious summer by this sun of York;\\n  And all the clouds that lour\\'d upon our house\\n  In the deep bosom of the ocean buried.\\n  Now are our brows bound with victorious wreaths;\\n  Our bruised arms hung up for monuments;\\n  Our stern alarums changed to merry meetings,\\n  Our dreadful marches to delightful measures.\\n  Grim-visaged war hath smooth\\'d his wrinkled front;\\n  And now, instead of mounting barded steeds\\n  To fright the souls of fearful adversaries,\\n  He capers nimbly in a lady\\'s chamber\\n  To the lascivious pleasing of a lute.\\n  But I, that am not shaped for sportive tricks,\\n  Nor made to court an amorous looking-glass;\\n  I, that am rudely stamp\\'d, and want love\\'s majesty\\n  To strut before a wanton ambling nymph;\\n  I, that am curtail\\'d of this fair proportion,\\n  docs = text_splitter.split_documents(documents)\\n🕵️ What’s Likely Going Wrong?\\nThe chunk_size=20 with chunk_overlap=5 is super small. You\\'re telling it to chop the text into tiny 20-character pieces, overlapping by 5 characters. But since your text isn’t massive, that’s likely resulting in only a few chunks—maybe even less than 5.\\n\\nPlus, it splits based on character count, not sentences or paragraphs.\\n\\n🧪 Try This Instead: Use Larger Chunks!\\nLet’s beef it up a bit:\\n\\nPython\\nCopy\\nEdit\\nfrom langchain_community.document_loaders import TextLoader\\nfrom langchain_text_splitters import CharacterTextSplitter\\n\\n# Load speech\\nloader = TextLoader(\"Speech.txt\")\\ndocuments = loader.load()\\n\\n# Use larger chunks to get more meaningful pieces\\ntext_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=50)\\ndocs = text_splitter.split_documents(documents)\\n\\n# Check number of splits\\nprint(f\"Total chunks: {len(docs)}\")\\nfor i, doc in enumerate(docs):\\n    print(f\"\\\\n--- Chunk {i+1} ---\\\\n{doc.page_content}\")\\n💡 Want Even Smarter Splitting?\\nIf you\\'re aiming for chunks that make more natural sense (like sentences or paragraphs), try using:\\n\\nPython\\nCopy\\nEdit\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=200,\\n    chunk_overlap=50,\\n    separators=[\"\\\\n\\\\n\", \"\\\\n\", \".\", \" \"]\\n)\\ndocs = text_splitter.split_documents(documents)\\nThis will keep your splits a bit more human-readable and context-aware!\\n\\n✅ Summary:\\nchunk_size=20 is too small—use 200+ for better results.\\n\\nUse RecursiveCharacterTextSplitter if you want natural sentence/paragraph breaks.\\n\\nsplit_documents() will always return a list of Document objects, even for one file.\\n\\nIf you want to split multiple files, use DirectoryLoader instead of TextLoader. Let me know if you need that too!\\n\\n\\n\\n\\n\\n\\n\\nYou said:\\naywhat does chunk\\n\\n\\n🧠 Real-Life Analogy: Netflix Series 🎬\\nImagine your speech is a story, and you want to split it into episodes.\\n\\nchunk_size = how long each episode is\\n\\nchunk_overlap = the “Previously on…” intro at the beginning of each new episode\\n\\n🔎 Why overlap matters?\\nIf an important sentence is cut in the middle, overlap helps the next chunk pick it up smoothly so the model doesn’t get confused.\\n\\n🧪 Example:\\nLet’s say we have this short text:\\n\\n“My parents impressed on me the value that you work hard for what you want in life.”\\n\\nCase 1: Small chunk size, no overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\nChunks:\\n\\n“My parents impress”\\n\\n“ed on me the value”\\n\\n“ that you work har”\\n\\n“d for what you wan”\\n\\n“t in life.”\\n\\nResult: You may lose meaning because words/sentences are broken.\\n\\nCase 2: Bigger chunk with overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 40\\nchunk_overlap = 10\\nChunks:\\n\\n“My parents impressed on me the value that yo”\\n\\n“value that you work hard for what you want ”\\n\\n“what you want in life.”\\n\\nNow, chunks carry better continuity and preserve meaning.\\n\\n🛠️ When to Use What?\\nUse Case\\tchunk_size\\tchunk_overlap\\nTiny docs\\tsmall (100–200)\\t10–20\\nLong speeches/books\\tbig (500–1000)\\t50–100\\nQA over documents\\tmedium (300–600)\\t50\\nSentence-sensitive tasks\\tsmall\\tmedium\\n\\n🧁 TL;DR – Key Takeaways\\nchunk_size = how big each text piece is\\n\\nchunk_overlap = how much text is shared between chunks\\n\\nSmall chunks: easier to process, but may lose context\\n\\nBig chunks: preserve meaning, but take more memory\\n\\nOverlap: preserves flow between chunks\\n\\nLet me know if you want a visual image or diagram to understand it better—I’ll whip one up! 😄\\n\\n\\n\\n\\n\\n\\n\\nYou said:\\nCase 1: Small chunk size, no overlap\\nPython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\nChunks:\\n\\n“My parents impress”\\n\\n“ed on me the value”\\n\\n“ that you work har”\\n\\n“d for what you wan”\\n\\n“t in life.”I don\\'t think the chunk size here is 20.\\n\\n\\nChatGPT said:\\nYou\\'re absolutely right, detective! 🕵️\\u200d♂️ Great catch — that example did not accurately show chunks of size 20 characters. Let\\'s fix that with real and precise math like good AI engineers 😎.\\n\\n🧮 Corrected Example:\\nLet’s take this sentence:\\n\\n\"My parents impressed on me the value that you work hard for what you want in life.\"\\n\\nThis sentence has 92 characters (including spaces and punctuation).\\n\\nNow let’s split it correctly using:\\n\\npython\\nCopy\\nEdit\\nchunk_size = 20\\nchunk_overlap = 0\\n✅ Real Chunks (each = 20 characters):\\n\"My parents impresse\" → 20 characters\\n\\n\"d on me the value t\" → next 20 characters\\n\\n\"hat you work hard f\"\\n\\n\"or what you want in\"\\n\\n\" life.\" → last remaining characters (less than 20)\\n\\n✅ Now it’s accurate!\\n\\n⚠️ Why It Matters\\nWhen you\\'re using CharacterTextSplitter, it slices your text based on character count, not words or sentences — unless you use other splitters like RecursiveCharacterTextSplitter or SentenceTextSplitter.\\n\\nSo, setting chunk_size=20 tells it:\\n\\n“Hey LangChain, give me 20 characters per slice, and don’t repeat anything if overlap = 0.”\\n\\n💡 Pro Tip\\nTo split by sentences or smartly, use this instead:\\n\\npython\\nCopy\\nEdit\\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=100,\\n    chunk_overlap=20,\\n    separators=[\"\\\\n\\\\n\", \"\\\\n\", \".\", \"!\", \"?\", \" \", \"\"]\\n)\\nThis tries to break at sentence ends or spaces, avoiding weird cuts in the middle of words like \"value t\".\\nLet me know if you\\'d like to compare word-based or sentence-based chunking too! I’ll chunk your worries away 😄✨\\n\\nTools\\nChatGPT can make mistakes. Check important info. See Cookie Preferences.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = TextLoader(\"speech.txt\")\n",
    "data = loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "508fd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "split = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba437387",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OllamaEmbeddings(model=\"nomic-embed-text\", model_kwargs={\"temperature\": 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03dacf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma.from_documents(\n",
    "    documents=split,\n",
    "    embedding=embedding\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19277209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1204e7f70>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a86b2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'🛠️ When to Use What?\\nUse Case\\tchunk_size\\tchunk_overlap\\nTiny docs\\tsmall (100–200)\\t10–20\\nLong speeches/books\\tbig (500–1000)\\t50–100\\nQA over documents\\tmedium (300–600)\\t50\\nSentence-sensitive tasks\\tsmall\\tmedium\\n\\n🧁 TL;DR – Key Takeaways\\nchunk_size = how big each text piece is\\n\\nchunk_overlap = how much text is shared between chunks\\n\\nSmall chunks: easier to process, but may lose context\\n\\nBig chunks: preserve meaning, but take more memory\\n\\nOverlap: preserves flow between chunks'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the main topic of the speech?\"\n",
    "docs = vector_db.similarity_search(query, k=3)\n",
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0d78ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db=Chroma.from_documents(documents=split, embedding=embedding, persist_directory=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46210f57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
