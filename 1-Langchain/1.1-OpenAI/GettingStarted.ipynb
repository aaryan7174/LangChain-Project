{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e0f3cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.document_loaders.text.TextLoader at 0x7ff108f176a0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Speech.txt\")\n",
    "loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23cd790e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Speech.txt'}, page_content='My parents impressed on me the value of that you work hard for what you want in life. That your word is your bond and you do what you say and keep your promise. That you treat people with respect. Show the values and morals in in the daily life. That is the lesson that we continue to pass on to our son.\\n\\nWe need to pass those lessons on to the many generations to follow. [Cheering] Because we want our children in these nations to know that the only limit to your achievement is the strength of your dreams and your willingness to work for them.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents=loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9d05d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1'}, page_content='Attention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser ∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring signiﬁcantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.0 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature.\\n1 Introduction\\nRecurrent neural networks, long short-term memory [12] and gated recurrent [7] neural networks\\nin particular, have been ﬁrmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [ 29, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [31, 21, 13].\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the ﬁrst Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefﬁcient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2'}, page_content='Recurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsigniﬁcant improvements in computational efﬁciency through factorization tricks [18] and conditional\\ncomputation [26], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 16]. In all but a few cases [22], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for signiﬁcantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2 Background\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difﬁcult to learn dependencies between distant positions [ 11]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 22, 23, 19].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [28].\\nTo the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [14, 15] and [8].\\n3 Model Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 29].\\nHere, the encoder maps an input sequence of symbol representations (x1,...,x n) to a sequence\\nof continuous representations z = (z1,...,z n). Given z, the decoder then generates an output\\nsequence (y1,...,y m) of symbols one element at a time. At each step the model is auto-regressive\\n[9], consuming the previously generated symbols as additional input when generating the next.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1 Encoder and Decoder Stacks\\nEncoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The ﬁrst is a multi-head self-attention mechanism, and the second is a simple, position-\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3'}, page_content='Figure 1: The Transformer - model architecture.\\nwise fully connected feed-forward network. We employ a residual connection [10] around each of\\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\\nLayerNorm(x+ Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder: The decoder is also composed of a stack of N = 6identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position ican depend only on the known outputs at positions less than i.\\n3.2 Attention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1 Scaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4'}, page_content='Scaled Dot-Product Attention\\n Multi-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices Kand V. We compute\\nthe matrix of outputs as:\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof 1√dk\\n. Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efﬁcient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by 1√dk\\n.\\n3.2.2 Multi-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneﬁcial to linearly project the queries, keys and values htimes with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\noutput values. These are concatenated and once again projected, resulting in the ﬁnal values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = ∑dk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5'}, page_content='MultiHead(Q,K,V ) = Concat(head1,..., headh)WO\\nwhere headi = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nWhere the projections are parameter matricesWQ\\ni ∈Rdmodel×dk , WK\\ni ∈Rdmodel×dk , WV\\ni ∈Rdmodel×dv\\nand WO ∈Rhdv×dmodel .\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h= 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3 Applications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[31, 2, 8].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation ﬂow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3 Position-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0,xW1 + b1)W2 + b2 (2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4 Embeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [24]. In the embedding layers, we multiply those weights by √dmodel.\\n3.5 Positional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6'}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\\nLayer Type Complexity per Layer Sequential Maximum Path Length\\nOperations\\nSelf-Attention O(n2 ·d) O(1) O(1)\\nRecurrent O(n·d2) O(n) O(n)\\nConvolutional O(k·n·d2) O(1) O(logk(n))\\nSelf-Attention (restricted) O(r·n·d) O(1) O(n/r)\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and ﬁxed [8].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel )\\nPE(pos,2i+1) = cos(pos/100002i/dmodel )\\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2πto 10000 ·2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any ﬁxed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [8] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4 Why Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1,...,x n) to another sequence of equal length (z1,...,z n), with xi,zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [11]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[31] and byte-pair [25] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7'}, page_content='the input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k<n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [ 15], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\\nconsiderably, to O(k·n·d+ n·d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side beneﬁt, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5 Training\\nThis section describes the training regime for our models.\\n5.1 Training Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the signiﬁcantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2 Hardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3 Optimizer\\nWe used the Adam optimizer [17] with β1 = 0.9, β2 = 0.98 and ϵ= 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate= d−0.5\\nmodel ·min(step_num−0.5,step_num·warmup_steps−1.5) (3)\\nThis corresponds to increasing the learning rate linearly for the ﬁrst warmup_stepstraining steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps= 4000.\\n5.4 Regularization\\nWe employ three types of regularization during training:\\nResidual Dropout We apply dropout [27] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8'}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU Training Cost (FLOPs)\\nEN-DE EN-FR EN-DE EN-FR\\nByteNet [15] 23.75\\nDeep-Att + PosUnk [32] 39.2 1.0 ·1020\\nGNMT + RL [31] 24.6 39.92 2.3 ·1019 1.4 ·1020\\nConvS2S [8] 25.16 40.46 9.6 ·1018 1.5 ·1020\\nMoE [26] 26.03 40.56 2.0 ·1019 1.2 ·1020\\nDeep-Att + PosUnk Ensemble [32] 40.4 8.0 ·1020\\nGNMT + RL Ensemble [31] 26.30 41.16 1.8 ·1020 1.1 ·1021\\nConvS2S Ensemble [8] 26.36 41.29 7.7 ·1019 1.2 ·1021\\nTransformer (base model) 27.3 38.1 3.3 · 1018\\nTransformer (big) 28.4 41.0 2.3 ·1019\\nLabel Smoothing During training, we employed label smoothing of value ϵls = 0.1 [30]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6 Results\\n6.1 Machine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α= 0.6 [31]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [31].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of ﬂoating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision ﬂoating-point capacity of each GPU 5.\\n6.2 Model Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9'}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN d model dff h d k dv Pdrop ϵls\\ntrain PPL BLEU params\\nsteps (dev) (dev) ×106\\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\\n(A)\\n1 512 512 5.29 24.9\\n4 128 128 5.00 25.5\\n16 32 32 4.91 25.8\\n32 16 16 5.01 25.4\\n(B) 16 5.16 25.1 58\\n32 5.01 25.4 60\\n(C)\\n2 6.11 23.7 36\\n4 5.19 25.3 50\\n8 4.88 25.5 80\\n256 32 32 5.75 24.5 28\\n1024 128 128 4.66 26.0 168\\n1024 5.12 25.4 53\\n4096 4.75 26.2 90\\n(D)\\n0.0 5.77 24.6\\n0.2 4.95 25.5\\n0.0 4.67 25.3\\n0.2 5.47 25.7\\n(E) positional embedding instead of sinusoids 4.92 25.7\\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneﬁcial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-ﬁtting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical\\nresults to the base model.\\n7 Conclusion\\nIn this work, we presented the Transformer, the ﬁrst sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained signiﬁcantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efﬁciently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10'}, page_content='References\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[9] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\\narXiv:1308.0850, 2013.\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[11] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient ﬂow in\\nrecurrent nets: the difﬁculty of learning long-term dependencies, 2001.\\n[12] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[13] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[14] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[15] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[16] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[17] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[18] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[19] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[20] Samy Bengio Łukasz Kaiser. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF2', 'creator': 'PyPDF', 'creationdate': '', 'subject': 'Neural Information Processing Systems http://nips.cc/', 'publisher': 'Curran Associates, Inc.', 'language': 'en-US', 'created': '2017', 'eventtype': 'Poster', 'description-abstract': 'The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.', 'title': 'Attention is All you Need', 'date': '2017', 'moddate': '2018-02-12T21:22:10-08:00', 'published': '2017', 'type': 'Conference Proceedings', 'firstpage': '5998', 'book': 'Advances in Neural Information Processing Systems 30', 'description': 'Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)', 'editors': 'I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett', 'author': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, Illia Polosukhin', 'lastpage': '6008', 'source': '/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11'}, page_content='[21] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n[22] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[23] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[24] Oﬁr Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[25] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[26] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[27] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overﬁtting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[28] Sainbayar Sukhbaatar, arthur szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[29] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[30] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[31] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[32] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n11')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader(\"/Users/aaryanrana/Desktop/Langchain/1-Langchain/3.2-DataIngestion/Attention.pdf\")\n",
    "loader\n",
    "pdf_documents=loader.load() \n",
    "pdf_documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aab6293d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ebbd6bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://www.copy.ai/bloggers'}, page_content='')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web based Loader \n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(\n",
    "    \"https://www.copy.ai/bloggers\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=[\"post-title\", \"post_content\", \"post-header\"]))\n",
    ")\n",
    "web_documents = loader.load()\n",
    "web_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72bd5e64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "docs= ArxivLoader(query=\"1706.03762\",load_max_docs=2).load()\n",
    "docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e7bf6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'LangChain', 'summary': \"LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\", 'source': 'https://en.wikipedia.org/wiki/LangChain'}, page_content='LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain\\'s use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\\n\\n\\n== History ==\\nLangChain was launched in October 2022 as an open source project by Harrison Chase, while working at machine learning startup Robust Intelligence. The project quickly garnered popularity, with improvements from hundreds of contributors on GitHub, trending discussions on Twitter, lively activity on the project\\'s Discord server, many YouTube tutorials, and meetups in San Francisco and London. In April 2023, LangChain had incorporated and the new startup raised over $20 million in funding at a valuation of at least $200 million from venture firm Sequoia Capital, a week after announcing a $10 million seed investment from Benchmark.\\nIn the third quarter of 2023, the LangChain Expression Language (LCEL) was introduced, which provides a declarative way to define chains of actions.\\nIn October 2023 LangChain introduced LangServe, a deployment tool to host LCEL code as a production-ready API.\\n\\n\\n== Capabilities ==\\nLangChain\\'s developers highlight the framework\\'s applicability to use-cases including chatbots, retrieval-augmented generation,  document summarization, and synthetic data generation.\\nAs of March 2023, LangChain included integrations with systems including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; Bash for summarization, syntax and semantics checking, and execution of shell scripts; multiple web scraping subsystems and templates; few-shot learning prompt generation support; finding and summarizing \"todo\" tasks in code; Google Drive documents, spreadsheets, and presentations summarization, extraction, and creation; Google Search and Microsoft Bing web search; OpenAI, Anthropic, and Hugging Face language models; iFixit repair guides and wikis search and summarization; MapReduce for question answering, combining documents, and question generation; N-gram overlap scoring; PyPDF, pdfminer, fitz, and pymupdf for PDF file text extraction and manipulation; Python and JavaScript code generation, analysis, and debugging; Milvus vector database to store and retrieve vector embeddings; Weaviate vector database to cache embedding and data objects; Redis cache database storage; Python RequestsWrapper and other methods for API requests; SQL and NoSQL databases including JSON support; Streamlit, including for logging; text mapping for k-nearest neighbors search; time zone conversion and calendar operations; tracing and recording stack symbols in threaded and asynchronous subprocess runs; and the Wolfram Alpha website and SDK. As of April 2023, it can read from more than 50 document types and data sources.\\n\\n\\n== LangChain tools ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\n\\nOfficial website\\nDiscord server support hub\\nLangchain-ai on GitHub'),\n",
       " Document(metadata={'title': 'Intelligent agent', 'summary': 'In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Intelligent_agent'}, page_content='In artificial intelligence, an intelligent agent is an entity that perceives its environment, takes actions autonomously to achieve goals, and may improve its performance through machine learning or by acquiring knowledge. Leading AI textbooks define artificial intelligence as the \"study and design of intelligent agents,\" emphasizing that goal-directed behavior is central to intelligence.\\nA specialized subset of intelligent agents, agentic AI (also known as an AI agent or simply agent), expands this concept by proactively pursuing goals, making decisions, and taking actions over extended periods, thereby exemplifying a novel form of digital agency.\\nIntelligent agents can range from simple to highly complex. A basic thermostat or control system is considered an intelligent agent, as is a human being, or any other system that meets the same criteria—such as a firm, a state, or a biome.\\nIntelligent agents operate based on an objective function, which encapsulates their goals. They are designed to create and execute plans that maximize the expected value of this function upon completion. For example, a reinforcement learning agent has a reward function, which allows programmers to shape its desired behavior. Similarly, an evolutionary algorithm\\'s behavior is guided by a fitness function.\\nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, and the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations.\\nIntelligent agents are often described schematically as abstract functional systems similar to computer programs. To distinguish theoretical models from real-world implementations, abstract descriptions of intelligent agents are called abstract intelligent agents. Intelligent agents are also closely related to software agents—autonomous computer programs that carry out tasks on behalf of users. They are also referred to using a term borrowed from economics: a \"rational agent\".\\n\\n\\n== Intelligent agents as the foundation of AI ==\\n\\nThe concept of intelligent agents provides a foundational lens through which to define and understand artificial intelligence. For instance, the influential textbook Artificial Intelligence: A Modern Approach (Russell & Norvig) describes:\\n\\nAgent: Anything that perceives its environment (using sensors) and acts upon it (using actuators). E.g., a robot with cameras and wheels, or a software program that reads data and makes recommendations.\\nRational Agent: An agent that strives to achieve the *best possible outcome* based on its knowledge and past experiences. \"Best\" is defined by a performance measure – a way of evaluating how well the agent is doing.\\nArtificial Intelligence (as a field): The study and creation of these rational agents.\\nOther researchers and definitions build upon this foundation. Padgham & Winikoff emphasize that intelligent agents should react to changes in their environment in a timely way, proactively pursue goals, and be flexible and robust (able to handle unexpected situations). Some also suggest that ideal agents should be \"rational\" in the economic sense (making optimal choices) and capable of complex reasoning, like having beliefs, desires, and intentions (BDI model). Kaplan and Haenlein offer a similar definition, focusing on a system\\'s ability to understand external data, learn from that data, and use what is learned to achieve goals through flexible adaptation.\\nDefining AI in terms of intelligent agents offers several key advantages:\\n\\nAvoids Philosophical Debates: It sidesteps arguments about whether AI is \"truly\" intelligent or conscious, like those raised by the Turing test or Searle\\'s Chinese Room. It focuses on behavior and goal achievement, not on replicating human thought.\\nObjective Testing: It provides a clear, scientific way to evaluate AI systems. Researchers can compare differen'),\n",
       " Document(metadata={'title': 'Model Context Protocol', 'summary': 'The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic to standardize the way artificial intelligence (AI) models like large language models (LLMs) integrate and share data with external tools, systems, and data sources. Technology writers have dubbed MCP “the USB-C of AI apps”, underscoring its goal of serving as a universal connector between language-model agents and external software. Designed to standardize context exchange between AI assistants and software environments, MCP provides a model-agnostic universal interface for reading files, executing functions, and handling contextual prompts. It was officially announced and open-sourced by Anthropic in November 2024, with subsequent adoption by major AI providers including OpenAI and Google DeepMind.', 'source': 'https://en.wikipedia.org/wiki/Model_Context_Protocol'}, page_content='The Model Context Protocol (MCP) is an open standard, open-source framework introduced by Anthropic to standardize the way artificial intelligence (AI) models like large language models (LLMs) integrate and share data with external tools, systems, and data sources. Technology writers have dubbed MCP “the USB-C of AI apps”, underscoring its goal of serving as a universal connector between language-model agents and external software. Designed to standardize context exchange between AI assistants and software environments, MCP provides a model-agnostic universal interface for reading files, executing functions, and handling contextual prompts. It was officially announced and open-sourced by Anthropic in November 2024, with subsequent adoption by major AI providers including OpenAI and Google DeepMind.\\n\\n\\n== Background ==\\nThe protocol was announced in November 2024 as an open standard for connecting AI assistants to data systems such as content repositories, business management tools, and development environments. It addresses the challenge of information silos and legacy systems that constrain even the most sophisticated AI models.\\nAnthropic introduced MCP to address the growing complexity of integrating LLMs with third-party systems. Before MCP, developers often had to build custom connectors for each data source or tool, resulting in what Anthropic described as an \"N×M\" data integration problem.\\nEarlier stop-gap approaches - such as OpenAI’s 2023 “function-calling” API and the ChatGPT plug-in framework - solved similar problems but required vendor-specific connectors. MCP’s authors note that the protocol deliberately re-uses the message-flow ideas of the Language Server Protocol (LSP) and is transported over JSON-RPC 2.0.\\nMCP was designed as a response to this challenge, offering a universal protocol for interfacing any AI assistant with any structured tool or data layer. The protocol was released with software development kits (SDK) in multiple programming languages, including Python, TypeScript, Java, and C#.\\n\\n\\n== Features ==\\nMCP defines a set of specifications for:\\n\\nData ingestion and transformation\\nContextual metadata tagging\\nModel interoperability across platforms\\nSecure, two-way connections between data sources and AI-powered tools\\nThe protocol enables developers to either expose their data through MCP servers or build AI applications (MCP clients) that connect to these servers. Key components include:\\n\\nProtocol specification and SDKs\\nLocal MCP server support in Claude Desktop apps\\nOpen-source repository of MCP servers\\n\\n\\n== Applications ==\\nMCP has been applied across a range of use cases in software development, business process automation, and natural language automation:\\n\\nSoftware development: Integrated development environments (IDE) such as Zed, platforms like Replit, and code intelligence tools such as Sourcegraph integrated MCP to give coding assistants access to real-time code context, useful in vibe coding.\\nEnterprise assistants: Companies like Block use MCP to allow internal assistants to retrieve information from proprietary documents, customer relationship management (CRM) systems, and company knowledge bases.\\nNatural language data access: Applications like AI2SQL leverage MCP to connect models with SQL databases, enabling plain-language information retrieval.\\nDesktop assistants: The Claude Desktop app runs local MCP servers to allow the assistant to read files or interact with system tools securely.\\nMulti-tool agents: MCP supports agentic AI workflows involving multiple tools (e.g., document lookup + messaging APIs), enabling chain-of-thought reasoning over distributed resources.\\n\\n\\n== Implementation ==\\nAnthropic maintains an open-source repository of reference MCP server implementations for popular enterprise systems including Google Drive, Slack, GitHub, Git, Postgres, Puppeteer and Stripe.\\nDevelopers can create custom MCP servers to connect proprietary systems or specialized data sources to AI models. These c'),\n",
       " Document(metadata={'title': 'FAISS', 'summary': 'FAISS (Facebook AI Similarity Search) is an open-source library for similarity search and clustering of vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. \\nFAISS is written in C++ with complete wrappers for Python and C. Some of the most useful algorithms are implemented on the GPU using CUDA.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/FAISS'}, page_content='FAISS (Facebook AI Similarity Search) is an open-source library for similarity search and clustering of vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning. \\nFAISS is written in C++ with complete wrappers for Python and C. Some of the most useful algorithms are implemented on the GPU using CUDA.\\n\\n\\n== Features ==\\nFAISS is organized as a toolbox that contains a variety of indexing methods that commonly involve a chain of components (preprocessing, compression, non-exhaustive search, etc.). The scope of the library is intentionally limited to focus on ANNS algorithmic implementation and to avoid facilities related to database functionality, distributed computing or feature extraction algorithms.\\nFAISS is designed with the following assumptions:\\n\\nPrimary data type for vector representation is FP32. The support of other floating-point formats, such as BF16 and FP16, is provided.\\nPrefer batches of input queries over a single input query for the search.\\nEmphasize on allowing users to write a fast prototyping code using its Python wrappers.\\nThe code should be as open as possible, so that users can access all the implementation details of the indexes.\\nThe following major categories of indexing methods are supported:\\n\\nBrute-force search\\nInverted-lists based indices\\nGraph indices, including  (Hierarchical navigable small world) HNSW and Navigating Spread-out Graph (NSG)\\nLocality-sensitive hashing (LSH)\\nThe following families of vector quantization methods are supported:\\n\\nBinary Quantization\\nScalar Quantization (SQ)\\nProduct Quantization (PQ), including Polysemous Codes, Optimized Product Quantization (OPQ) and Quicker ADC (PQFastScan)\\nAdditive Quantization (AQ), including Residual Quantization (RQ) and Local Search Quantization (LSQ)\\nNeural Quantization, including QINCO\\nFAISS focuses on euclidean distance and inner product distance for floating-point data. The limited support of other distances (manhattan distance, Lp distance, etc.) is also available.\\nFAISS code supports multithreading via OpenMP, utilizes BLAS via OpenBLAS or Intel MKL, and also uses custom SIMD kernels for x86 and ARM Neon CPUs.\\nBesides the similarity search, FAISS provides the following useful facilities:\\n\\nk-means clustering\\nRandom-matrix rotations for spreading the variance over all the dimensions without changing the measured distances\\nPrincipal component analysis\\nData deduplication, which is especially useful for image datasets.\\nFAISS has a standalone Vector Codec functionality for the lossy compression of vectors, allowing to trade the representation accuracy for the binary size.\\n\\n\\n== Applications ==\\nTypical FAISS applications include  recommender systems, data mining, text retrieval and content moderation.\\nFAISS was reported to index 1.5 trillion 144-dimensional vectors for internal Meta Platforms applications.\\nFAISS is used in vector databases as a core component of a search engine (OpenSearch, Milvus, Vearch).\\nFAISS is often considered as a baseline in similarity search benchmarks.\\nFAISS has an integration with Haystack, LangChain frameworks.\\nVarious advanced code snippets for FAISS can be found on its snippets wiki page and case studies wiki page.\\n\\n\\n== See also ==\\n\\nNearest neighbor search\\nSimilarity search\\nVector database\\nVector quantization\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website\\nfaiss on GitHub\\nOfficial FAISS wiki\\nGuidelines to choose a FAISS index\\nAutofaiss - automatically create Faiss knn indices with the most optimal similarity search parameters'),\n",
       " Document(metadata={'title': 'Milvus (vector database)', 'summary': 'Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Milvus_(vector_database)'}, page_content='Milvus is a distributed vector database developed by Zilliz. It is available as both open-source software and a cloud service.\\nMilvus is an open-source project under LF AI & Data Foundation distributed under the Apache License 2.0.\\n\\n\\n== History ==\\nMilvus has been developed by Zilliz since 2017.\\nMilvus joined Linux Foundation as an incubation project in January 2020 and became a graduate in June 2021. The details about its architecture and possible applications were presented on ACM SIGMOD Conference in 2021\\nMilvus 2.0, a major redesign of the whole product with a new architecture, was released in January 2022.\\n\\n\\n== Features ==\\n\\n\\n=== Similarity search ===\\nMajor similarity search related features that are available in the active 2.4.x Milvus branch:\\n\\nIn-memory, on-disk and GPU indices,\\nSingle query, batch query and range query search,\\nSupport of sparse vectors, binary vectors, JSON and arrays,\\nFP32, FP16 and BF16 data types,\\nEuclidean distance, inner product distance and cosine distance support for floating-point data,\\nHamming distance and jaccard distance for binary data,\\nSupport of graph indices (including HNSW), Inverted-lists based indices and a brute-force search.\\nSupport of vector quantization for lossy input data compression, including product quantization (PQ) and scalar quantization (SQ), that trades stored data size for accuracy,\\nRe-ranking.\\nMilvus similarity search engine relies on heavily-modified forks of third-party open-source similarity search libraries, such as Faiss, DiskANN and hnswlib.\\nMilvus includes optimizations for I/O data layout, specific to graph search indices.\\n\\n\\n=== Database ===\\nAs a database, Milvus provides the following features:\\n\\nColumn-oriented database\\nFour supported data consistency levels, including strong consistency and eventual consistency.\\nData sharding\\nStreaming data ingestion, which allows to process and ingest data in real-time as it arrives\\nA dynamic schema, which allows inserting the data without a predefined schema\\nIndependent storage and compute layers\\nMulti-tenancy scenarios (database-oriented, collection-oriented, partition-oriented)\\nMemory-mapped data storage\\nRole-based access control\\nMulti-vector and hybrid search\\n\\n\\n=== Deployment options ===\\nMilvus can be deployed as an embedded database, standalone server, or distributed cluster. Zillis Cloud offers a fully managed version.\\n\\n\\n=== GPU support ===\\nMilvus provides GPU accelerated index building and search using Nvidia CUDA technology via Nvidia RAFT library, including a recent GPU-based graph indexing algorithm Nvidia CAGRA\\n\\n\\n=== Integration ===\\nMilvus provides official SDK clients for Java, NodeJS, Python and Go. An additional C# SDK client was contributed by Microsoft. The database can integrate with Prometheus and Grafana for monitoring and alerts, frameworks Haystack and LangChain, IBM Watsonx, and OpenAI models.\\n\\n\\n== See also ==\\n\\nNearest neighbor search\\nSimilarity search\\nVector database\\nVector embedding\\nVector quantization\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'DataStax', 'summary': 'DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/DataStax'}, page_content=\"DataStax, Inc. is a real-time data for AI company based in Santa Clara, California. Its product Astra DB is a cloud database-as-a-service based on Apache Cassandra. DataStax also offers DataStax Enterprise (DSE), an on-premises database built on Apache Cassandra, and Astra Streaming, a messaging and event streaming cloud service based on Apache Pulsar. As of June 2022, the company has roughly 800 customers distributed in over 50 countries.\\n\\n\\n== History ==\\nDataStax was built on the open source NoSQL database Apache Cassandra. Cassandra was initially developed internally at Facebook to handle large data sets across multiple servers, and was released as an Apache open source project in 2008. In 2010, Jonathan Ellis and Matt Pfeil left Rackspace, where they had worked with Cassandra, to launch Riptano in Austin, Texas. Ellis and Pfeil later renamed the company DataStax, and moved its headquarters to Santa Clara, California.\\nThe company went on to create its own enterprise version of Cassandra, a NoSQL database called DataStax Enterprise (DSE).\\nIn 2019, Chet Kapoor was named the company's new CEO, taking over from Billy Bosworth.\\n\\nIn May 2020, DataStax released Astra DB, a DBaaS for Cassandra applications. In November 2020, DataStax released K8ssandra, an open source distribution of Cassandra on Kubernetes. In December 2020, DataStax released Stargate, an open source data API gateway.\\nAfter acquiring streaming event vendor Kesque in January 2021, the company launched Luna Streaming, a data streaming platform for Apache Pulsar. DataStax then rebuilt the Kesque technology into Astra Streaming. The Astra Streaming cloud service became generally available on June 29, 2022. With the release, the company added API-level support for messaging tools Apache Kafka, RabbitMQ and Java Message Service, in addition to Apache Pulsar. Astra Streaming can connect to a larger data platform by utilizing DataStax's Astra DB cloud service.\\nStarting in 2023, DataStax began incorporating artificial intelligence and machine learning into its platform. In January 2023, the company acquired Kaskada, developer of a platform that helps organizations use data for AI applications. DataStax made the formerly proprietary Kaskada technology open source, and integrated it into its Luna ML service, which was launched on May 4, 2023. With the acquisition, former Kaskada CEO Davor Bonaci was named DataStax chief technology officer and executive vice president.\\nOn May 24, 2023, DataStax announced that it would be partnering with ThirdAI to bring large language models to DSE and AstraDB, to help developers develop generative AI applications.\\nIn June 2023, the company announced the development of a GPT-based schema translator in its Astra Streaming cloud service. The Astra Streaming GPT Schema Translator uses generative AI to automatically generate schema mappings, to enable data integration and interoperability between multiple systems and data sources.\\nOn July 18, 2023, the company announced a partnership with Google to make semantic search available in its Astra DB cloud database for developers building generative AI applications.\\nOn September 13, 2023, DataStax launched the LangStream open source project, which works with Astra DB and supports vector databases including Milvus and Pinecone. LangStream enables developers to better work with streaming data sources, using Apache Kafka technology and generative AI to help build event-driven architectures.\\nIn November 2023, DataStax announced RAGStack, a simplified commercial offering for RAG (retrieval-augmented generation) based on LangChain and Astra DB vector search.\\nOn February 25, 2025, IBM announced its intention to acquire DataStax.\\n\\n\\n== Products ==\\n\\n\\n=== Astra DB ===\\nAstra DB is available on cloud services such as Microsoft Azure, Amazon Web Services, and Google Cloud Platform. In February 2021, DataStax announced the serverless version of Astra DB, offering developers pay-as-you-go data.\\nIn March 2022, DataStax \"),\n",
       " Document(metadata={'title': 'Sentence embedding', 'summary': \"In natural language processing, a sentence embedding is a representation of a sentence as a vector of numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions; this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Sentence_embedding'}, page_content=\"In natural language processing, a sentence embedding is a representation of a sentence as a vector of numbers which encodes meaningful semantic information.\\nState of the art embeddings are based on the learned hidden layer representation of dedicated sentence transformer models. BERT pioneered an approach involving the use of a dedicated [CLS] token prepended to the beginning of each sentence inputted into the model; the final hidden state vector of this token encodes information about the sentence and can be fine-tuned for use in sentence classification tasks. In practice however, BERT's sentence embedding with the [CLS] token achieves poor performance, often worse than simply averaging non-contextual word embeddings. SBERT later achieved superior sentence embedding performance by fine tuning BERT's [CLS] token embeddings through the usage of a siamese neural network architecture on the SNLI dataset. \\nOther approaches are loosely based on the idea of distributional semantics applied to sentences. Skip-Thought trains an encoder-decoder structure for the task of neighboring sentences predictions; this has been shown to achieve worse performance than approaches such as InferSent or SBERT. \\nAn alternative direction is to aggregate word embeddings, such as those returned by Word2vec, into sentence embeddings. The most straightforward approach is to simply compute the average of word vectors, known as continuous bag-of-words (CBOW). However, more elaborate solutions based on word vector quantization have also been proposed. One such approach is the vector of locally aggregated word embeddings (VLAWE), which demonstrated performance improvements in downstream text classification tasks.\\n\\n\\n== Applications ==\\nIn recent years, sentence embedding has seen a growing level of interest due to its applications in natural language queryable knowledge bases through the usage of vector indexing for semantic search. LangChain for instance utilizes sentence transformers for purposes of indexing documents. In particular, an indexing is generated by generating embeddings for chunks of documents and storing (document chunk, embedding) tuples. Then given a query in natural language, the embedding for the query can be generated. A top k similarity search algorithm is then used between the query embedding and the document chunk embeddings to retrieve the most relevant document chunks as context information for question answering tasks. This approach is also known formally as retrieval-augmented generation\\nThough not as predominant as BERTScore, sentence embeddings are commonly used for sentence similarity evaluation which sees common use for the task of optimizing a Large language model's generation parameters is often performed via comparing candidate sentences against reference sentences. By using the cosine-similarity of the sentence embeddings of candidate and reference sentences as the evaluation function, a grid-search algorithm can be utilized to automate hyperparameter optimization .\\n\\n\\n== Evaluation ==\\nA way of testing sentence encodings is to apply them on Sentences Involving Compositional Knowledge (SICK) corpus\\nfor both entailment (SICK-E) and relatedness (SICK-R).\\nIn  the best results are obtained using a BiLSTM network trained on the Stanford Natural Language Inference (SNLI) Corpus. The Pearson correlation coefficient for SICK-R is 0.885 and the result for SICK-E is 86.3. A slight improvement over previous scores is presented in: SICK-R: 0.888 and SICK-E: 87.8 using a concatenation of bidirectional Gated recurrent unit.\\n\\n\\n== See also ==\\nDistributional semantics\\nWord embedding\\n\\n\\n== External links ==\\n\\nInferSent sentence embeddings and training code\\nUniversal Sentence Encoder\\nLearning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning\\n\\n\\n== References ==\"),\n",
       " Document(metadata={'title': 'Retrieval-augmented generation', 'summary': 'Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs . Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.', 'source': 'https://en.wikipedia.org/wiki/Retrieval-augmented_generation'}, page_content='Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM\\'s pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\\nRAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don\\'t exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\\nRAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs . Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\\nThe term RAG was first introduced in a 2020 research paper from Meta.\\n\\n\\n== RAG and LLM Limitations ==\\nLLMs can provide incorrect information. For example, when Google first demonstrated its LLM tool \"Google Bard\", the LLM provided incorrect information about the James Webb Space Telescope. This error contributed to a $100 billion decline in the company’s stock value. RAG is used to prevent these errors, but it does not solve all the problems. For example, LLMs can generate misinformation even when pulling from factually correct sources if they misinterpret the context. MIT Technology Review gives the example of an AI-generated response stating, \"The United States has had one Muslim president, Barack Hussein Obama.\" The model retrieved this from an academic book rhetorically titled Barack Hussein Obama: America’s First Muslim President? The LLM did not \"know\" or \"understand\" the context of the title, generating a false statement.\\nLLMs with RAG are programmed to prioritize new information. This technique has been called \"prompt stuffing.\" Without \"prompt stuffing\", the LLM\\'s input is generated by a user; with prompt stuff, additional relevant context is added to this input to guide the model’s response. This approach provides the LLM with key information early in the prompt, encouraging it to prioritize the supplied data over pre-existing training knowledge.\\n\\n\\n== Process ==\\nRetrieval-Augmented Generation (RAG) enhances large language models (LLMs) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set. AWS states, \"RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses\" (indexing). This approach reduces reliance on static datasets, which can quickly become outdated. When a user submits a query, RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the model’s response (retrieval). Ars Technica notes that \"when new information becomes available, rather than having to retrain the model, all that’s needed is to augment the model’s external knowledge base with the updated information\" (augmentation). By dynamically integrating relevant data, RAG enables LLMs to generate more informed and contextually grounded responses (generation). IBM states that \"in the gener'),\n",
       " Document(metadata={'title': 'List of supermarket chains in Germany', 'summary': 'This is a list of current German supermarket chains.', 'source': 'https://en.wikipedia.org/wiki/List_of_supermarket_chains_in_Germany'}, page_content='This is a list of current German supermarket chains.\\n\\n\\n== German supermarket chains ==\\n\\n\\n== See also ==\\n\\nList of supermarket chains\\nList of supermarket chains in Europe\\nIndian grocery online in Germany\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Markov chain', 'summary': 'In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \"What happens next depends only on the state of affairs now.\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). Markov processes are named in honor of the Russian mathematician Andrey Markov.\\nMarkov chains have many applications as statistical models of real-world processes. They provide the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in areas including Bayesian statistics, biology, chemistry, economics, finance, information theory, physics, signal processing, and speech processing.\\nThe adjectives Markovian and Markov are used to describe something that is related to a Markov process.', 'source': 'https://en.wikipedia.org/wiki/Markov_chain'}, page_content='In probability theory and statistics, a Markov chain or Markov process is a stochastic process describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. Informally, this may be thought of as, \"What happens next depends only on the state of affairs now.\" A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). Markov processes are named in honor of the Russian mathematician Andrey Markov.\\nMarkov chains have many applications as statistical models of real-world processes. They provide the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in areas including Bayesian statistics, biology, chemistry, economics, finance, information theory, physics, signal processing, and speech processing.\\nThe adjectives Markovian and Markov are used to describe something that is related to a Markov process.\\n\\n\\n== Principles ==\\n\\n\\n=== Definition ===\\nA Markov process is a stochastic process that satisfies the Markov property (sometimes characterized as \"memorylessness\"). In simpler terms, it is a process for which predictions can be made regarding future outcomes based solely on its present state and—most importantly—such predictions are just as good as the ones that could be made knowing the process\\'s full history. In other words, conditional on the present state of the system, its future and past states are independent.\\nA Markov chain is a type of Markov process that has either a discrete state space or a discrete index set (often representing time), but the precise definition of a Markov chain varies. For example, it is common to define a Markov chain as a Markov process in either discrete or continuous time with a countable state space (thus regardless of the nature of time), but it is also common to define a Markov chain as having discrete time in either countable or continuous state space (thus regardless of the state space).\\n\\n\\n=== Types of Markov chains ===\\nThe system\\'s state space and time parameter index need to be specified. The following table gives an overview of the different instances of Markov processes for different levels of state space generality and for discrete time v. continuous time:\\n\\nNote that there is no definitive agreement in the literature on the use of some of the terms that signify special cases of Markov processes. Usually the term \"Markov chain\" is reserved for a process with a discrete set of times, that is, a discrete-time Markov chain (DTMC), but a few authors use the term \"Markov process\" to refer to a continuous-time Markov chain (CTMC) without explicit mention. In addition, there are other extensions of Markov processes that are referred to as such but do not necessarily fall within any of these four categories (see Markov model). Moreover, the time index need not necessarily be real-valued; like with the state space, there are conceivable processes that move through index sets with other mathematical constructs. Notice that the general state space continuous-time Markov chain is general to such a degree that it has no designated term.\\nWhile the time parameter is usually discrete, the state space of a Markov chain does not have any generally agreed-on restrictions: the term may refer to a process on an arbitrary state space. However, many applications of Markov chains employ finite or countably infinite state spaces, which have a more straightforward statistical analysis. Besides time-index and state-space parameters, there are many other variations, extensions and generalizations (see Variations). For simplicity, most of this article concentrates on the discrete-time, discrete state-space case, unless mentioned otherwise.\\n\\n\\n=== Transitions ===\\nThe changes '),\n",
       " Document(metadata={'title': 'List of The Late Late Show with Craig Ferguson episodes', 'summary': \"The Late Late Show with Craig Ferguson is an American late-night talk show that aired weeknights at 12:37 am (Los Angeles time) on CBS in the United States from January 3, 2005, to December 19, 2014. The hour-long show was hosted by Scottish American comedian, actor and author Craig Ferguson, with his animatronic robot skeleton sidekick Geoff Peterson (voiced by Josh Robert Thompson), and featuring Secretariat, a pantomime horse. The show's writers and other staff appeared in skits and as themselves occasionally, with show producer Michael Naidus becoming a regular. 2,058 episodes were produced.\", 'source': 'https://en.wikipedia.org/wiki/List_of_The_Late_Late_Show_with_Craig_Ferguson_episodes'}, page_content=\"The Late Late Show with Craig Ferguson is an American late-night talk show that aired weeknights at 12:37 am (Los Angeles time) on CBS in the United States from January 3, 2005, to December 19, 2014. The hour-long show was hosted by Scottish American comedian, actor and author Craig Ferguson, with his animatronic robot skeleton sidekick Geoff Peterson (voiced by Josh Robert Thompson), and featuring Secretariat, a pantomime horse. The show's writers and other staff appeared in skits and as themselves occasionally, with show producer Michael Naidus becoming a regular. 2,058 episodes were produced.\\n\\n\\n== 2005 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2006 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2007 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\nThere were no other episodes produced in November as well as all of December due to the 2007–08 Writers Guild of America strike.\\n\\n\\n== 2008 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2009 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2010 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2011 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2012 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2013 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== 2014 ==\\n\\n\\n=== January ===\\n\\n\\n=== February ===\\n\\n\\n=== March ===\\n\\n\\n=== April ===\\n\\n\\n=== May ===\\n\\n\\n=== June ===\\n\\n\\n=== July ===\\n\\n\\n=== August ===\\n\\n\\n=== September ===\\n\\n\\n=== October ===\\n\\n\\n=== November ===\\n\\n\\n=== December ===\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nCraig Ferguson on Twitter\\nThe Late Late Show with Craig Ferguson at IMDb\"),\n",
       " Document(metadata={'title': \"List of Kickin' It episodes\", 'summary': \"Kickin' It is an American comedy television series created by Jim O'Doherty that aired on Disney XD from June 13, 2011 to March 25, 2015. The series stars Leo Howard, Dylan Riley Snyder, Mateo Arias, Olivia Holt, Alex Christian Jones, and Jason Earles.\", 'source': 'https://en.wikipedia.org/wiki/List_of_Kickin%27_It_episodes'}, page_content=\"Kickin' It is an American comedy television series created by Jim O'Doherty that aired on Disney XD from June 13, 2011 to March 25, 2015. The series stars Leo Howard, Dylan Riley Snyder, Mateo Arias, Olivia Holt, Alex Christian Jones, and Jason Earles.\\n\\n\\n== Series overview ==\\n\\n\\n== Episodes ==\\n\\n\\n=== Season 1 (2011–12) ===\\n\\n\\n=== Season 2 (2012) ===\\n\\n\\n=== Season 3 (2013–14) ===\\n\\n\\n=== Season 4 (2014–15) ===\\n\\n\\n== See also ==\\nList of Kickin' It characters\\n\\n\\n== References ==\"),\n",
       " Document(metadata={'title': 'List of Sydney Taylor Book Award recipients', 'summary': \"The Sydney Taylor Book Award, established in 1968, recognizes the best in Jewish children's literature. Medals are awarded annually for outstanding books that authentically portray the Jewish experience.\\nThis list provides Sydney Taylor Book Award recipients, not including manuscript and body-of-work awards. The Children's Book Award was uncategorized from 1968 to 1980, after which two categories were presented: Younger Readers and Older Readers. In 1985, a Teen Reader category was introduced, though it has not been presented annually.  In 2009 and 2010, an All Ages Award was also presented.\", 'source': 'https://en.wikipedia.org/wiki/List_of_Sydney_Taylor_Book_Award_recipients'}, page_content='The Sydney Taylor Book Award, established in 1968, recognizes the best in Jewish children\\'s literature. Medals are awarded annually for outstanding books that authentically portray the Jewish experience.\\nThis list provides Sydney Taylor Book Award recipients, not including manuscript and body-of-work awards. The Children\\'s Book Award was uncategorized from 1968 to 1980, after which two categories were presented: Younger Readers and Older Readers. In 1985, a Teen Reader category was introduced, though it has not been presented annually.  In 2009 and 2010, an All Ages Award was also presented.\\n\\n\\n== Award categories ==\\n\\n\\n=== Uncategorized (1968–1980) ===\\n\\n\\n=== Younger Reader / Picture Book (1981–present) ===\\nIn 2020, the \"Younger Reader\" category was redefined as \"Picture Book\".\\n\\n\\n=== Older Reader (1981–2019) ===\\nIn 2020, the \"Older Reader\" category was redefined as \"Middle Grade\".\\n\\n\\n=== Teen Reader / Young Adult (1985–present) ===\\nThe \"Teen Reader\" category was first issued in 1985, though it was released intermittently.  In 2020, the category was renamed as the \"Young Adult\" category.\\n\\n\\n=== All Ages (2009–2010) ===\\n\\n\\n=== Manuscript Award (1985–present) ===\\n\\n\\n=== Body-of-Work Award ===\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'List of German films of the 1970s', 'summary': 'This is a list of the most notable films produced in Cinema of Germany in the 1970s.\\nFor an alphabetical list of articles on West German films see Category:West German films. For East German films made during the decade see List of East German films.', 'source': 'https://en.wikipedia.org/wiki/List_of_German_films_of_the_1970s'}, page_content='This is a list of the most notable films produced in Cinema of Germany in the 1970s.\\nFor an alphabetical list of articles on West German films see Category:West German films. For East German films made during the decade see List of East German films.\\n\\n\\n== 1970 ==\\n\\n\\n== 1971 ==\\n\\n\\n== 1972 ==\\n\\n\\n== 1973 ==\\n\\n\\n== 1974 ==\\n\\n\\n== 1975 ==\\n\\n\\n== 1976 ==\\n\\n\\n== 1977 ==\\n\\n\\n== 1978 ==\\n\\n\\n== 1979 ==\\n\\n\\n== Notes ==\\n\\n\\n=== References ===\\nBrowning, John Edgar; Picart, Caroline Joan (2010). Dracula in Visual Media:Film, Television, Comic Book and Electronic Game Appearances, 1921-2010. McFarland. ISBN 978-0786433650.\\n\\n\\n== External links ==\\nGerman film at the Internet Movie Database (maintains separate lists for West Germany and East Germany)'),\n",
       " Document(metadata={'title': 'List of The Weekly with Charlie Pickering episodes', 'summary': 'The Weekly with Charlie Pickering is an Australian news satire series on the ABC. The series premiered on 22 April 2015, and Charlie Pickering as host with Tom Gleeson, Adam Briggs, Kitty Flanagan (2015–2018) in the cast, and Judith Lucy joined the series in 2019. The first season consisted of 20 episodes and concluded on 22 September 2015. The series was renewed for a second season on 18 September 2015, which premiered on 3 February 2016. The series was renewed for a third season with Adam Briggs joining the team and began airing from 1 February 2017. The fourth season premiered on 2 May 2018 at the later timeslot of 9:05pm to make room for the season return of Gruen at 8:30pm, and was signed on for 20 episodes. \\nFlanagan announced her departure from The Weekly With Charlie Pickering during the final episode of season four, but returned for The Yearly with Charlie Pickering special in December 2018. \\nIn 2019, the series was renewed for a fifth season with Judith Lucy announced as a new addition to the cast as a \"wellness expert\".\\nThe show was pre-recorded in front of an audience in ABC\\'s Ripponlea studio on the same day of its airing from 2015 to 2017. In 2018, the fourth season episodes were pre-recorded in front of an audience at the ABC Southbank Centre studios. In 2020, the show was filmed without a live audience due to COVID-19 pandemic restrictions and comedian Luke McGregor joined the show as a regular contributor. Judith Lucy did not return in 2021 and Zoë Coombs Marr joined as a new cast member in season 7 with the running joke that she was fired from the show in episode one yet she kept returning to work for the show.', 'source': 'https://en.wikipedia.org/wiki/List_of_The_Weekly_with_Charlie_Pickering_episodes'}, page_content='The Weekly with Charlie Pickering is an Australian news satire series on the ABC. The series premiered on 22 April 2015, and Charlie Pickering as host with Tom Gleeson, Adam Briggs, Kitty Flanagan (2015–2018) in the cast, and Judith Lucy joined the series in 2019. The first season consisted of 20 episodes and concluded on 22 September 2015. The series was renewed for a second season on 18 September 2015, which premiered on 3 February 2016. The series was renewed for a third season with Adam Briggs joining the team and began airing from 1 February 2017. The fourth season premiered on 2 May 2018 at the later timeslot of 9:05pm to make room for the season return of Gruen at 8:30pm, and was signed on for 20 episodes. \\nFlanagan announced her departure from The Weekly With Charlie Pickering during the final episode of season four, but returned for The Yearly with Charlie Pickering special in December 2018. \\nIn 2019, the series was renewed for a fifth season with Judith Lucy announced as a new addition to the cast as a \"wellness expert\".\\nThe show was pre-recorded in front of an audience in ABC\\'s Ripponlea studio on the same day of its airing from 2015 to 2017. In 2018, the fourth season episodes were pre-recorded in front of an audience at the ABC Southbank Centre studios. In 2020, the show was filmed without a live audience due to COVID-19 pandemic restrictions and comedian Luke McGregor joined the show as a regular contributor. Judith Lucy did not return in 2021 and Zoë Coombs Marr joined as a new cast member in season 7 with the running joke that she was fired from the show in episode one yet she kept returning to work for the show.\\n\\n\\n== Series overview ==\\n\\n\\n== Episodes ==\\n\\n\\n=== Season 1 (2015) ===\\n\\n\\n=== Season 2 (2016) ===\\n\\n\\n=== Season 3 (2017) ===\\n\\n\\n=== Season 4 (2018) ===\\n\\n\\n=== Season 5 (2019) ===\\n\\n\\n=== Season 6 (2020) ===\\n\\n\\n=== Season 7 (2021) ===\\n\\n\\n=== Season 8 (2022) ===\\n\\n\\n=== Season 9 (2023) ===\\n\\n\\n=== Season 10 (2024) ===\\n\\n\\n=== Season 11 (2025) ===\\n\\n\\n=== The Yearly with Charlie Pickering ===\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'List of The Legend of Qin episodes', 'summary': 'Below is a list of the Episodes for the Chinese CG animated TV series, The Legend of Qin. See the List of The Legend of Qin Characters for their roles and alternative names.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/List_of_The_Legend_of_Qin_episodes'}, page_content='Below is a list of the Episodes for the Chinese CG animated TV series, The Legend of Qin. See the List of The Legend of Qin Characters for their roles and alternative names.\\n\\n\\n== Series overview ==\\n\\n\\n== Season 1: 2007 ==\\nThe title for the season is: 百步飞剑 Bai Bu Fei Jian (The Hundred Pace Flying Sword), which is also\\nGe Nie\\'s signature move.  It consists of only 10 episodes.\\n\\n\\n== Season 2: 2008 ==\\nThe title for the season is Night Ends - Daybreak（Tianming）. It consists of 18 episodes.\\n\\n\\n== Season 3: 2009 ==\\nThe title for the season is: 诸子百家 Zhu Zi Bai Jia (Masters of the Hundreds of Schools), the word \"Hundred\" here means many instead of an accurate number. The expression is the classical description for the diverse intellectual life of the Warring States period of Chinese history.\\nThe third season was finished in 2009 and aired in 2010, it consists of 34 episodes.\\n\\n\\n== Season 4: 2012 ==\\nThe title for the season is 万里长城 (wànlǐ chángchéng) and it consists of 37 episodes.\\n\\n\\n== Season 5: 2014 ==\\n\\n\\n== Specials ==\\nBirdsong in Hollow Valley: 2014\\nThis special consists of three episodes which tells the story of Baifeng and his friend Moya who were assassins for General Ji Wuye, but who turn against him after Baifeng falls for a female assassin, Nong Yu.\\n\\nUnder Luosheng Tang\\nThis special tells of a young Taoist who joins the Yin-Yang School to find his sister.\\n\\nLady Xiang Descends\\nThis special is a story about a wounded man who finds himself caught up in the lives of twin sisters and the man that they both fell in love with.\\n\\n\\n== See also ==\\nList of The Legend of Qin characters\\nThe Legend of Qin animated film\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'National Register of Historic Places listings in Hennepin County, Minnesota', 'summary': 'This list is of the properties and historic districts which are designated on the National Register of Historic Places or that were formerly so designated, in Hennepin County, Minnesota; there are 195 entries as of November 2024. A significant number of these properties are a result of the establishment of Fort Snelling, the development of water power at Saint Anthony Falls, and the thriving city of Minneapolis that developed around the falls. Many historic sites outside the Minneapolis city limits are associated with pioneers who established missions, farms, and schools in areas that are now suburbs in that metropolitan area.', 'source': 'https://en.wikipedia.org/wiki/National_Register_of_Historic_Places_listings_in_Hennepin_County,_Minnesota'}, page_content='This list is of the properties and historic districts which are designated on the National Register of Historic Places or that were formerly so designated, in Hennepin County, Minnesota; there are 195 entries as of November 2024. A significant number of these properties are a result of the establishment of Fort Snelling, the development of water power at Saint Anthony Falls, and the thriving city of Minneapolis that developed around the falls. Many historic sites outside the Minneapolis city limits are associated with pioneers who established missions, farms, and schools in areas that are now suburbs in that metropolitan area.\\n\\n\\n== Historical background ==\\n\\nFather Louis Hennepin was the first European explorer to visit and name Saint Anthony Falls, the tallest waterfall on the Mississippi River, in 1680. While the falls were familiar to the Ojibwe and Sioux Indians who lived in the area, Father Hennepin spread word of the falls when he returned to France in 1683. The land east of the Mississippi came under England\\'s control in 1763, and then became American territory after the American Revolutionary War in 1783. After the Louisiana Purchase in 1803, the western side of the falls became American territory as well.\\nZebulon Pike explored the Mississippi River in 1805 and made a treaty with the Sioux to acquire land on either side of the Mississippi River from its confluence with the Minnesota River to Saint Anthony Falls. The United States did not do much to occupy the land until 1819, when Lieutenant Colonel Henry Leavenworth was ordered to establish a military post in the area. The following year, Colonel Josiah Snelling established a permanent fort at a blufftop site overlooking Pike Island and the confluence of the Mississippi and Minnesota Rivers. The fort, first named Fort Saint Anthony and later Fort Snelling, became an island of civilization in the wilderness.\\nIn 1837, Franklin Steele established a claim for the land on the east side of Saint Anthony Falls. Within the next ten years, he established a sawmill at the falls, and lumbermen from the north began cutting trees and sending them to Steele\\'s sawmill. In 1849, Steele subdivided his property and filed a plat for the town of Saint Anthony. Sawmilling and early flour milling attempts proved successful, and by 1855 the fledgling town of Saint Anthony had more than three thousand residents. The west side of the river was part of the Fort Snelling military reservation until it was released for development in 1854. In 1849, John H. Stevens obtained 160 acres (0.65 km2) of land on the west side of the falls in exchange for maintaining a ferry at the falls. Hennepin County was established in 1852, and the settlement on the west side of the river was given the name Minneapolis, as coined by Charles Hoag. The two towns prospered as a result of industries and businesses based around the falls, but business was better on the west side of the falls. Minneapolis incorporated as a city in 1867, and three years later it merged with the village of Saint Anthony.\\nEventually, flour mills overtook sawmills as a dominant industry at the falls. In 1860, flour production stood at 30,000 barrels; it reached 256,100 barrels in 1869. By 1874, Charles A. Pillsbury and Company owned five mills at the falls, and in 1879, Washburn-Crosby Company (now General Mills) owned four mills. The former Washburn \"A\" Mill building on the west side of the falls exploded on May 2, 1878, but its owners quickly rebuilt the west side district, including a new, larger Washburn \"A\" Mill. Meanwhile, in 1880, Pillsbury began building the huge Pillsbury \"A\" Mill on the east side of the falls. It had a capacity of 4,000 barrels per day when it first opened. Improvements in milling technology made it possible to grind the tougher spring wheat into a finer product, producing Minnesota \"patent\" flour, the finest bread flour in the world at that time. By 1900, Minneapolis was grinding 14.1 percent of the world\\'s grain.\\n\\n\\n'),\n",
       " Document(metadata={'title': 'Itaconic acid', 'summary': 'Itaconic acid is an organic compound with the formula CH2=CCH2(CO2H)2. With two carboxyl groups, it is classified as a dicarboxylic acid.  It is a non-toxic white solid that is soluble in water and several organic solvents.  It plays several roles in biology.', 'source': 'https://en.wikipedia.org/wiki/Itaconic_acid'}, page_content=\"Itaconic acid is an organic compound with the formula CH2=CCH2(CO2H)2. With two carboxyl groups, it is classified as a dicarboxylic acid.  It is a non-toxic white solid that is soluble in water and several organic solvents.  It plays several roles in biology.\\n\\n\\n== Reactions ==\\nUpon heating, itaconic acid converts to its anhydride.\\nAs a dicarboxylic acid, itaconic acid has two pKa's.  At pH levels above 7, itaconic acid exists as its double negatively charged form, termed itaconate. \\nAs an α,β-unsaturated carbonyl compound, itaconic acid is a good Michael acceptor.  Thus, nucleophiles add across the C=C bond. \\n\\nCH2=CCH2(CO2H)2  +  R2P(O)H   →   R2P(O)CH2−CHCH2(CO2H)2 (R = organic group).\\nThis reaction is the means by which the fire retarding chemical 9,10-Dihydro-9-oxa-10-phosphaphenanthrene-10-oxide can be incorporated into polymers.\\n\\n\\n== Production ==\\nIn 1836, Samuel Baup discovered itaconic acid as a by-product in a dry distillation of citric acid. In the late 1920s, itaconic acid was isolated from a fungus in the Aspergillus genus of fungi The dry distillation forms itaconic anhydride, which then is hydrolyzed. Since the 1960s, however, it has been produced commercially by fermenting glucose, molasses, or another abundant carbon sources by a fungus such as Aspergillus itaconicus, Aspergillus terreus, or Ustilago maydis have also been investigated. One generally accepted route by which fungi make itaconate is through the tricarboxylic acid cycle pathway. This pathway forms cis-aconitate which is converted to itaconate by cis-aconitate-decarboxylase. Animal cells also make itaconate by an enzyme-catalyzed reaction from cis-aconitate, an intermediate metabolite in the tricarboxylic acid cycle, (i.e., TCA cycle). The itaconate-producing reaction is stimulated when the TCA cycle is suppressed. \\nUstilago maydis  makes itaconic acid from trans-aconitate, catalyzed by aconitate delta-isomerase. The trans-aconitate product is decarboxylated to itaconate by trans-aconitate decarboxylase (i.e., TAD1, an enzyme found in Ustilago maydis) Itaconate has also been obtained by fermenting the fungi Yarrowia lipolytica with glucose, various species of Candida fungi with glucose, Ustilago vetiveriae fungus with glycerol, and various species of Aspergillus niger fungi with glucose, sorbitol, or sorbitol plus xylose mixture. Fermenting Escherichia coli bacteria with glucose, xylose, glycerol, or starch and Corynebacterium glutamicum bacteria with glucose or urea also affords itaconic acid.  Ustilago maydis has, however, been genetically engineered to increase its itaconic acid production,\\n\\n\\n== History ==\\nIn the 1930s itaconate was shown to have bactericidal actions.  In 2011, Strelko et al. reported that itaconate was produced by two mammalian immortalized cell lines, cultured mouse VM-M3 brain tumor cells and RAW 264.7 mouse macrophages, and by macrophages isolated from mice. This group also showed that stimulation of mouse macrophages with the bacterial toxin, lipopolysaccharide (i.e., LPS, also termed endotoxin), increased their production and secretion of itaconate. In 2013, Michelucci et al. revealed the biosynthesis pathway that makes itaconate in mammals. These publications were followed by numerous others focused on the biology of itaconate and certain itaconate-like compounds as regulars of various cellular responses in animals and possibly humans.\\n\\n\\n== Biology of Itaconate ==\\nBiological studies focus on itaconate's physiological and pathological functions. \\n\\n\\n=== Cells making itaconate ===\\nThe major cell types that normally make itaconate in response to stressful conditions are hematological cells such as the macrophages located in various tissues and the monocytes located in the bone marrow and blood. These cells are phagocytes, i.e., cells that ingulf microorganisms, dead or seriously injured cells, and foreign particles all of which cause inflammatory responses. Itaconate is also produced by certain myeloid-derived suppressor cells \"),\n",
       " Document(metadata={'title': 'List of German films of the 1990s', 'summary': 'This is a list of the most notable films produced in Cinema of Germany in the 1990s.\\nFor an alphabetical list of articles on German films see Category:1990s German films.', 'source': 'https://en.wikipedia.org/wiki/List_of_German_films_of_the_1990s'}, page_content='This is a list of the most notable films produced in Cinema of Germany in the 1990s.\\nFor an alphabetical list of articles on German films see Category:1990s German films.\\n\\n\\n== 1990 ==\\n\\n\\n== 1991 ==\\n\\n\\n== 1992 ==\\n\\n\\n== 1993 ==\\n\\n\\n== 1994 ==\\n\\n\\n== 1995 ==\\n\\n\\n== 1996 ==\\n\\n\\n== 1997 ==\\n\\n\\n== 1998 ==\\n\\n\\n== 1999 ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nGerman film at the Internet Movie Database (maintains separate lists for West Germany and East Germany)'),\n",
       " Document(metadata={'title': 'List of people who adopted matrilineal surnames', 'summary': \"This is a list of notable people who have changed, adopted or adjusted their surnames based on a mother's or grandmother's maiden name. Included are people who changed their legal names and people who created personal or professional pseudonyms. Under longstanding Western custom and law, children are customarily given the father's surname, except for children born outside marriage, who often carry their mother's family names. In mediaeval times where a great family died out in the male line, an alternative male heir to the estates was selected as one of the younger sons of a daughter, who was required by the bequest to adopt, by royal licence, in lieu of his patronymic, his maternal surname and coat of arms for himself and his descendants. This was also the origin of double-barrelled surnames, where the paternal surname was partially retained, or resurrected by a later generation. The compliance with the terms of the bequest was essential to avoid challenge by another potential heir in the lawcourts. In the 1970s some women began to adopt their mother's maiden name as their legal surnames. People in Sweden have recently begun adopting maternal line surnames in an effort to broaden the number of last names in the country. Such practices add considerable difficulties to the study of genealogy and family history.\", 'source': 'https://en.wikipedia.org/wiki/List_of_people_who_adopted_matrilineal_surnames'}, page_content='This is a list of notable people who have changed, adopted or adjusted their surnames based on a mother\\'s or grandmother\\'s maiden name. Included are people who changed their legal names and people who created personal or professional pseudonyms. Under longstanding Western custom and law, children are customarily given the father\\'s surname, except for children born outside marriage, who often carry their mother\\'s family names. In mediaeval times where a great family died out in the male line, an alternative male heir to the estates was selected as one of the younger sons of a daughter, who was required by the bequest to adopt, by royal licence, in lieu of his patronymic, his maternal surname and coat of arms for himself and his descendants. This was also the origin of double-barrelled surnames, where the paternal surname was partially retained, or resurrected by a later generation. The compliance with the terms of the bequest was essential to avoid challenge by another potential heir in the lawcourts. In the 1970s some women began to adopt their mother\\'s maiden name as their legal surnames. People in Sweden have recently begun adopting maternal line surnames in an effort to broaden the number of last names in the country. Such practices add considerable difficulties to the study of genealogy and family history.\\n\\n\\n== Stage names ==\\nMany actors and other entertainers elect to add or include their mothers\\' maiden names in their adopted stage names. The book How to be a Working Actor: The Insider\\'s Guide to Finding Jobs in Theater, Film, and Television advises aspiring performers to consider changing their names, noting that \"if [your birth name] is difficult to spell, pronounce, or remember, it may not be the name you want for your professional career.\" It goes on to suggest: \"If you want to retain a connection to your family, try using your mother\\'s maiden name or the name of a revered relative.\"\\nA person\\'s mother\\'s maiden name is used by many financial institutions as a key piece of information to validate a customer\\'s identity. In 2005, researchers showed that the common practice of using a mother\\'s maiden name as the basis for a stage name could be exploited to entice people to reveal that name and other details that could allow fraudsters to steal their identities. Researchers asked a random sample of people on London streets a series of questions, beginning with \"What is your name?\" They then engaged in conversation about theatre, asked people if they knew how actors choose their stage names, then told them that stage names were typically a combination of the name of a pet and the mother\\'s maiden name. Next the participants were asked what their stage names would be; 94% responded by revealing both their mother\\'s maiden name and a pet\\'s name.\\n\\n\\n== List ==\\n\\n\\n=== A ===\\nJanet Achurch, British actress, was born Janet Sharp; her mother died during childbirth and she was reared by her father William Prior Sharp, an insurance agent. She later adopted her maternal grandparents\\' surname (Achurch) as her professional name.\\nEdie Adams, American actress and singer, was born Edith Elizabeth Enke to Sheldon Alonzo Enke and his wife, Ada Dorothy (née Adams), later adopting her mother\\'s maiden name as her professional name.\\nMaude Adams, American actress, was born Maude Ewing Kiskadden (or Maude Ewing Adams Kiskadden) and adopted her mother\\'s maiden name of Adams as her stage name.\\nAndrás Adorján, Hungarian author and chess grandmaster; born as András Jocha (or Jocha András), he adopted his mother\\'s maiden name, Adorján, at the age of 18, in 1968.\\nTheodor Adorno, German sociologist, philosopher and musicologist known for his critical theory of society, was born Theodor Ludwig Wiesengrund to Oscar Alexander Wellington (1870–1946) and Maria Calvelli-Adorno della Piana (1865–1952). His mother wanted her son\\'s paternal surname to be supplemented by the addition of her own surname/maiden name, Adorno. His earliest publications carried the name Th'),\n",
       " Document(metadata={'title': 'Robert Lang Studios', 'summary': 'Robert Lang Studios is a recording studio in Shoreline, Washington, United States. Numerous bands have recorded at Robert Lang Studios since 1974 including Nirvana, Alice in Chains, Foo Fighters, Soundgarden, Dave Matthews Band, Death Cab for Cutie, Heart, Sir Mix-A-Lot, Peter Frampton, Candlebox, and Bush.', 'source': 'https://en.wikipedia.org/wiki/Robert_Lang_Studios'}, page_content='Robert Lang Studios is a recording studio in Shoreline, Washington, United States. Numerous bands have recorded at Robert Lang Studios since 1974 including Nirvana, Alice in Chains, Foo Fighters, Soundgarden, Dave Matthews Band, Death Cab for Cutie, Heart, Sir Mix-A-Lot, Peter Frampton, Candlebox, and Bush.\\n\\n\\n== History ==\\nRobert Lang quit his job as a Boeing hydrofoil TIG welder to pursue a career in recording, first establishing the studio in 1974 in the garage of a beach house in Shoreline, Washington, near Seattle. The studio remained garage-based for the first seven years, with early projects including Seattle\\'s Franklin High School jazz lab, which included a 15 year-old saxophonist by the name of Kenneth Gorelick (who would later achieve fame as Kenny G), as well as Albert Collins.\\nIn 1982, Lang purchased the property. He began gradually excavating the earth beneath the house over the course of the next several years to create new subterranean reinforced concrete rooms for the studio. In April 1992, Candlebox recorded two songs that would be included on their debut album at Robert Lang Studios, including their hit song \"Far Behind\" In 1993, 11 years after construction began, the studio\\'s new live room, complete with marble and granite floors and natural stone walls, was finally finished.\\nIn late January 1994, Nirvana recorded their last known studio recording, \"You Know You\\'re Right\" at Robert Lang Studios. In October of the same year, Dave Grohl, formerly of Nirvana, booked the studio, which was close to his house, for six days, for his own musical project. The resulting album was the self-titled debut album that resulted in Grohl\\'s forming Foo Fighters. \\nIn October 1998, frontman Layne Staley recorded vocals for two new Alice In Chains songs, \"Get Born Again\" and \"Died\". The songs, which were released on the Music Bank box set in 1999, would be the last the singer would record with the band.\\nIn 2006 Peter Frampton recorded the track \"Blowin\\' Smoke\" with Pearl Jam drummer Matt Cameron and guitarist Mike McCready for his album Fingerprints. In 2008, Death Cab for Cutie recorded their highest-charting album, Narrow Stairs at the studio.\\nIn 2014 Foo Fighters and Death Cab for Cutie\\'s Ben Gibbard returned to Robert Lang Studios for episode 7 of the HBO rockumentary mini-series Foo Fighters Sonic Highways. During the episode, Foo Fighters and Gibbard recorded the song \"Subterranean\".\\nArtists recording at Robert Lang Studios have included Heart, Sir Mix-A-Lot, Alice In Chains, Dave Matthews, Bush, Brandi Carlile, Queensrÿche, Train, Portugal. The Man, and The Blood Brothers.\\n\\n\\n== Selected albums recorded at Robert Lang Studios ==\\nSir Mix-a-Lot - Seminar (1989)\\nMichael Gettel - Skywatching (1993)\\nFoo Fighters - Foo Fighters (1994)\\nPretty Girls Make Graves - Good Health (2001) \\nMastodon - Leviathan (2004)\\nDeath Cab for Cutie - Narrow Stairs (2008)\\nHockey Dad - Blend Inn (2018)\\n\\n\\n== See also ==\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website'),\n",
       " Document(metadata={'title': 'The Texas Chain Saw Massacre (2023 video game)', 'summary': \"The Texas Chain Saw Massacre is a 2023 asymmetrical survival horror game originally developed by Sumo Nottingham and published by Gun Interactive. It is based on the 1974 film of the same name.\\nThe game's main mode features four victims attempting to escape the family of cannibals before they catch and kill them. The game's cast stars Kane Hodder as Leatherface (who also played the character as a stunt double in 1990's Leatherface: The Texas Chainsaw Massacre III) and Edwin Neal as the voice of the Hitchhiker (reprising the role from the original film).\\nThe Texas Chain Saw Massacre was released for PlayStation 4, PlayStation 5, Windows, Xbox One and Xbox Series X/S on August 18, 2023, also releasing on Xbox Game Pass on the same day. The game received generally positive reviews upon release, with praise directed to its faithfulness to the 1974 film and its unique four versus three gameplay, though criticism towards its matchmaking and technical issues.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/The_Texas_Chain_Saw_Massacre_(2023_video_game)'}, page_content=\"The Texas Chain Saw Massacre is a 2023 asymmetrical survival horror game originally developed by Sumo Nottingham and published by Gun Interactive. It is based on the 1974 film of the same name.\\nThe game's main mode features four victims attempting to escape the family of cannibals before they catch and kill them. The game's cast stars Kane Hodder as Leatherface (who also played the character as a stunt double in 1990's Leatherface: The Texas Chainsaw Massacre III) and Edwin Neal as the voice of the Hitchhiker (reprising the role from the original film).\\nThe Texas Chain Saw Massacre was released for PlayStation 4, PlayStation 5, Windows, Xbox One and Xbox Series X/S on August 18, 2023, also releasing on Xbox Game Pass on the same day. The game received generally positive reviews upon release, with praise directed to its faithfulness to the 1974 film and its unique four versus three gameplay, though criticism towards its matchmaking and technical issues.\\n\\n\\n== Plot ==\\nThe main mode of the game take place in April 1973, five months before the event of the 1974 film. The plot revolves around Ana Flores and her college friends, who go searching for her missing sister, Maria, near the fictional town of Newt, Texas. The group is ultimately captured by the Slaughter family, a group of cannibal maniacs.\\nThe events of the game are considered canon by the developers; however, although each match may end with one or more of the victims escaping and surviving, developers have confirmed that in the official timeline of the franchise, it is considered that none of them survived the family: they are all dead by the time of the events of the 1974 film.\\nThe Rush Week mode takes place in 1978, after the event of the main mode. The plot revolves around Johnny who enters a sorority house in Granger Hill, Texas, with the intention of killing the girls living in it. The plot of this mode is also considered canon and was validated by Kim Henkel.\\n\\n\\n== Modes ==\\n\\n\\n=== Main mode ===\\n\\n\\n==== Gameplay ====\\nPlayers begin a match by taking on the role of either a family member or a victim, with a total of three family members and four victims being playable for a total of seven players per match. A match is set on one of six maps: the Slaughter family house, a gas station, a slaughterhouse, a mill, Nancy's house, and a graveyard. The family house and gas station are locations featured in the film whereas the slaughterhouse, the mill, the graveyard, and Nancy's house are new locales. Each map, except for the graveyard, has a day and a night variation, altering the lighting and atmosphere. A feature on each map is the basement where all the victims and Leatherface begin the match.\\nAfter escaping their restraints, the victims must first escape the basement to reach various methods of escape. Victims can escape via one of the four exits which are always present in every map. Family members meanwhile must track and kill the victims; if enough damage is dealt to a victim, they are immediately killed and are removed from the match. Family members must also feed Grandpa, a stationary character who feeds on the blood of victims and blood buckets around the map. After he is awoken by enough noise being made by victims, he will occasionally shriek, revealing the outlines of any victim who is moving to all family members. A match is completed when all victims either escape or are killed.\\nPlayers gain experience from each round and can spend earned skill points into each character's specific skill tree. Perks and attributes are unlocked via the skill tree, allowing each character to be customized and fit a certain play style.\\n\\n\\n==== Characters ====\\n† This symbol denotes characters available through DLCs.\\nThere are eight family members to play as, each with unique play styles and abilities.\\n\\nLeatherface is a chainsaw-wielding maniac who is unique in that he starts the game in the basement with the victims. His chainsaw allows him to destroy obstacles and deal great damage t\"),\n",
       " Document(metadata={'title': 'List of European Academy Award winners and nominees', 'summary': 'This is a list of European Academy Award winners and nominees, which includes people born and/or raised in Europe and people born outside of Europe who are citizens of European countries.', 'source': 'https://en.wikipedia.org/wiki/List_of_European_Academy_Award_winners_and_nominees'}, page_content='This is a list of European Academy Award winners and nominees, which includes people born and/or raised in Europe and people born outside of Europe who are citizens of European countries.\\n\\n\\n== Best Picture ==\\n\\n\\n== Best Director ==\\n\\n\\n== Best Actor in a Leading Role ==\\n\\n\\n== Best Actress in a Leading Role ==\\n\\n\\n== Best Actor in a Supporting Role ==\\n\\n\\n== Best Actress in a Supporting Role ==\\n\\n\\n== Best Original Screenplay ==\\n\\n\\n== Best Adapted Screenplay ==\\nDuring certain ceremonies the Oscars only gave out one screenplay awards. If a screenplay is original, it will be noted as such.\\n\\n\\n== Best Story ==\\n\\n\\n== Best Animated Feature ==\\n\\n\\n== Best Animated Short Film ==\\n\\n\\n== Best Cinematography ==\\n\\n\\n== Best Film Editing ==\\n\\n\\n== Best Makeup and Hairstyling ==\\n\\n\\n== Best Original Score ==\\n\\n\\n== Best International Feature Film ==\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'History of CBS', 'summary': 'CBS Broadcasting, Inc. (CBS; originally the Columbia Broadcasting System) is an American English-language commercial broadcast television and radio network owned by Paramount Global through the CBS Entertainment Group. Along with ABC and NBC, CBS is one of the traditional \"Big Three\" American television networks.\\nCBS was founded as a radio network in 1927 and then expanded to television in the 1940s. Although it primarily remained an independent, publicly-traded company (NYSE: CBS) throughout most of the 20th century, Paramount Pictures temporarily held a 49% ownership stake from 1929 to 1932. However, in 1995 the Westinghouse Electric Corporation acquired the company, becoming CBS Corporation (after selling certain assets). In 2000, CBS sold again to the original incarnation of Viacom (formed as a spin-off of CBS in 1971, which acquired Paramount Pictures in 1994). In 2005, Viacom split itself into two separate companies and re-established CBS Corporation. However, National Amusements controlled both CBS and the second incarnation of Viacom until 2019, when both companies agreed to re-merge to become ViacomCBS. In 2022, ViacomCBS changed its name to Paramount Global after Paramount Pictures.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/History_of_CBS'}, page_content='CBS Broadcasting, Inc. (CBS; originally the Columbia Broadcasting System) is an American English-language commercial broadcast television and radio network owned by Paramount Global through the CBS Entertainment Group. Along with ABC and NBC, CBS is one of the traditional \"Big Three\" American television networks.\\nCBS was founded as a radio network in 1927 and then expanded to television in the 1940s. Although it primarily remained an independent, publicly-traded company (NYSE: CBS) throughout most of the 20th century, Paramount Pictures temporarily held a 49% ownership stake from 1929 to 1932. However, in 1995 the Westinghouse Electric Corporation acquired the company, becoming CBS Corporation (after selling certain assets). In 2000, CBS sold again to the original incarnation of Viacom (formed as a spin-off of CBS in 1971, which acquired Paramount Pictures in 1994). In 2005, Viacom split itself into two separate companies and re-established CBS Corporation. However, National Amusements controlled both CBS and the second incarnation of Viacom until 2019, when both companies agreed to re-merge to become ViacomCBS. In 2022, ViacomCBS changed its name to Paramount Global after Paramount Pictures.\\n\\n\\n== Early radio years ==\\nThe origins of CBS date back to January 27, 1927, with the creation of the United Independent Broadcasters network in Chicago by New York City talent agent Arthur Judson. The fledgling network soon needed additional investors, and the Columbia Phonograph Company, manufacturers of Columbia Records, rescued it in April 1927. Now the Columbia Phonographic Broadcasting System, the network went to air under its new name on May 18, 1927, with a presentation by the Howard L. Barlow Orchestra from flagship station WOR in Newark, and fifteen affiliates.\\nOperational costs were steep, particularly the payments to AT&T for use of its landlines, and by the end of 1927, Columbia Phonograph wanted out. In early 1928 Judson sold the network to brothers Isaac and Leon Levy, owners of the network\\'s Philadelphia affiliate WCAU, and their partner Jerome Louchheim. None of the three were interested in assuming day-to-day management of the network, so they installed wealthy 26-year-old William S. Paley, son of a Philadelphia cigar family and in-law of the Levys, as president. With the record company out of the picture, Paley quickly streamlined the corporate name to \"Columbia Broadcasting System\". He believed in the power of radio advertising since his family\\'s La Palina cigars had doubled their sales after young William convinced his elders to advertise on radio. By September 1928, Paley bought out the Louchheim share of CBS and became its majority owner with 51% of the business.\\n\\n\\n=== Turnaround: Paley\\'s first year ===\\nDuring Louchheim\\'s brief regime, Columbia paid $410,000 to Alfred H. Grebe\\'s Atlantic Broadcasting Corporation (ABC) for the small Brooklyn station WABC (no relation to the current WABC), which would become the network\\'s flagship station. WABC was quickly upgraded, and the signal relocated to 860 kHz. The physical plant was also relocated to Steinway Hall on West 57th Street in Manhattan, where much of CBS\\'s programming would originate. By the turn of 1929, the network had 47 affiliates.\\nPaley moved right away to put his network on a firmer financial footing. In the fall of 1928, he entered into talks with Adolph Zukor of Paramount Pictures, who planned to move into radio in response to RCA\\'s forays into motion pictures with the advent of talkies. The deal came to fruition in September 1929; Paramount acquired 49% of CBS in return for a block of its stock worth $3.8 million at the time. The agreement specified that Paramount would buy that same stock back for a flat $5 million by March 1, 1932, provided that CBS had earned $2 million during 1931 and 1932. For a brief time, there was talk that the network might be renamed \"Paramount Radio\", but it only lasted a month as the 1929 stock market crash sent all stock value '),\n",
       " Document(metadata={'title': 'List of Rock Band Network songs', 'summary': 'The Rock Band Network in the music video games Rock Band 2 and Rock Band 3 supported downloadable songs for the Xbox 360, PlayStation 3, and Wii veins throughout the consoles\\' respective online services. The Rock Band Network Store became publicly available on March 4, 2010, for all Xbox 360 players in selected countries (US, Canada, UK, France, Italy, Germany, Spain, Sweden, and Singapore). Rock Band Network songs became available on the PlayStation 3 in five song intervals through their own Rock Band Network Store on April 22, 2010. Starting on April 12, 2011, up to 10 songs were added weekly to the PlayStation 3 platform until June 14, 2011, when it reverted to five song intervals. Also, starting on June 14, 2011, PlayStation 3 Rock Band Network songs were only compatible with Rock Band 3.  Rock Band Network became available on the Wii in six to 10 song intervals from September 7, 2010 to January 18, 2011. Rock Band Network songs were exclusive to the Xbox 360 for 30 days, after which a selection of songs were made available on the PlayStation 3 and Wii. As of January 18, 2011, no further Rock Band Network songs would be released on the Wii platform due to Nintendo\\'s small online install base, limited demand for the songs and the significant amount of work each song needed to convert to the Wii.\\nPlayers can download songs (and free demos of the songs if being used on the Xbox 360) on a track-by-track basis. Unlike a song released through the regular music store, there are limitations to where the song can be used. Network songs will not appear as a song within the various \"Mystery Setlist\" challenges within Tour mode (except on Wii, where they are treated as regular DLC), though users can add Network songs to \"Make a Setlist\".  Users can also use Network songs in Quickplay modes.  Network songs cannot be played in the head-to-head modes, as this would require Network authors to also balance note tracks for these game modes. Songs can be practiced through Practice Mode, but unlike Harmonix-authored songs, which include hooks to allow the user to practice specific sections of a song, Network songs are not authored with these phrase hooks and can only be practiced in percentage based segments (i.e. short songs would get 10% increments, longer would get 5%, etc.).\\nWith the release of Rock Band Network 2.0, creators could add songs with harmony vocals, standard and pro mode keyboard tracks, and pro drum tracks, as well as mark specific sections for practicing and the end-of-song breakdown. Support for pro guitar and bass was not included in RBN 2.0 due to the complexity of authoring such tracks and the small base of pro guitar users/testers early on. With the formal launch of RBN 2.0 on February 15, 2011, the previous version of the network was shut down, ending RBN support for Rock Band 2.', 'source': 'https://en.wikipedia.org/wiki/List_of_Rock_Band_Network_songs'}, page_content='The Rock Band Network in the music video games Rock Band 2 and Rock Band 3 supported downloadable songs for the Xbox 360, PlayStation 3, and Wii veins throughout the consoles\\' respective online services. The Rock Band Network Store became publicly available on March 4, 2010, for all Xbox 360 players in selected countries (US, Canada, UK, France, Italy, Germany, Spain, Sweden, and Singapore). Rock Band Network songs became available on the PlayStation 3 in five song intervals through their own Rock Band Network Store on April 22, 2010. Starting on April 12, 2011, up to 10 songs were added weekly to the PlayStation 3 platform until June 14, 2011, when it reverted to five song intervals. Also, starting on June 14, 2011, PlayStation 3 Rock Band Network songs were only compatible with Rock Band 3.  Rock Band Network became available on the Wii in six to 10 song intervals from September 7, 2010 to January 18, 2011. Rock Band Network songs were exclusive to the Xbox 360 for 30 days, after which a selection of songs were made available on the PlayStation 3 and Wii. As of January 18, 2011, no further Rock Band Network songs would be released on the Wii platform due to Nintendo\\'s small online install base, limited demand for the songs and the significant amount of work each song needed to convert to the Wii.\\nPlayers can download songs (and free demos of the songs if being used on the Xbox 360) on a track-by-track basis. Unlike a song released through the regular music store, there are limitations to where the song can be used. Network songs will not appear as a song within the various \"Mystery Setlist\" challenges within Tour mode (except on Wii, where they are treated as regular DLC), though users can add Network songs to \"Make a Setlist\".  Users can also use Network songs in Quickplay modes.  Network songs cannot be played in the head-to-head modes, as this would require Network authors to also balance note tracks for these game modes. Songs can be practiced through Practice Mode, but unlike Harmonix-authored songs, which include hooks to allow the user to practice specific sections of a song, Network songs are not authored with these phrase hooks and can only be practiced in percentage based segments (i.e. short songs would get 10% increments, longer would get 5%, etc.).\\nWith the release of Rock Band Network 2.0, creators could add songs with harmony vocals, standard and pro mode keyboard tracks, and pro drum tracks, as well as mark specific sections for practicing and the end-of-song breakdown. Support for pro guitar and bass was not included in RBN 2.0 due to the complexity of authoring such tracks and the small base of pro guitar users/testers early on. With the formal launch of RBN 2.0 on February 15, 2011, the previous version of the network was shut down, ending RBN support for Rock Band 2.\\n\\n\\n== Pricing ==\\nPrices for Rock Band Networks songs were set by the parties involved with authoring and submitting the song, and could be set at either 80, 160 or 240 Microsoft Points ($1, 2, or 3, respectively.) The artist retained 30% of this cost, with the remaining 70% of each sale split between Harmonix and Microsoft (although the exact ratios of that distribution are unknown).\\n\\n\\n== Complete list of available songs ==\\nThe following songs have been released to the Rock Band Network. Songs released prior to March 15, 2011, on the Xbox 360 and June 14, 2011, on the PlayStation 3 were available for Rock Band 2 and Rock Band 3, while songs released on or after those dates were only available in Rock Band 3. All songs could only be released individually; All songs utilize the song\\'s master recording.  New songs were released every day exclusively for Xbox Live for 30 days.  After 30 days, songs were eligible to be brought over to the Wii and PlayStation 3. Dates listed are the initial release of songs on Xbox Live. Starting March 4, 2010, all downloadable songs were available in US, Canada, UK, France, Italy, Germany, Spain, Sweden, and Singap')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "loader = WikipediaLoader(\"LangChain\")\n",
    "wiki_documents = loader.load()\n",
    "wiki_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fad60f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
